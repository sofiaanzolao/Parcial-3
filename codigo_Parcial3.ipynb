{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sofiaanzolao/Parcial-3/blob/main/codigo_Parcial3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHUhEwAeBjYZ"
      },
      "source": [
        "# **Parcial 3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgXomao7bHym"
      },
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# IMPORTS BÁSICOS\n",
        "# ============================================\n",
        "!pip install -q langchain langchain-community sentence-transformers faiss-cpu pypdf transformers accelerate bitsandbytes\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "from pypdf import PdfReader\n",
        "import os, textwrap, shutil, numpy as np, torch\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "\n",
        "try:\n",
        "    from google.colab import files\n",
        "    COLAB = True\n",
        "except ImportError:\n",
        "    COLAB = False\n",
        "\n",
        "# ============================================\n",
        "# SUBIDA DE PDFs\n",
        "# ============================================\n",
        "if COLAB:\n",
        "    print(\"Sube tus PDFs...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "BASE_FOLDER = \"/content/BASES_DE_DATOS\"\n",
        "os.makedirs(BASE_FOLDER, exist_ok=True)\n",
        "\n",
        "for f in os.listdir(\"/content\"):\n",
        "    full = os.path.join(\"/content\", f)\n",
        "    if os.path.isfile(full) and f.lower().endswith(\".pdf\"):\n",
        "        shutil.move(full, os.path.join(BASE_FOLDER, f))\n",
        "\n",
        "print(\"PDFs cargados:\", os.listdir(BASE_FOLDER))\n",
        "# ============================================\n",
        "# LECTURA Y CHUNKING\n",
        "# ============================================\n",
        "\n",
        "def load_pdfs_and_chunk(folder_path, chunk_size=500, chunk_overlap=100):\n",
        "    corpus = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if not filename.lower().endswith(\".pdf\"):\n",
        "            continue\n",
        "\n",
        "        pdf_path = os.path.join(folder_path, filename)\n",
        "        reader = PdfReader(pdf_path)\n",
        "\n",
        "        for page_num, page in enumerate(reader.pages):\n",
        "            try:\n",
        "                page_text = page.extract_text() or \"\"\n",
        "            except:\n",
        "                page_text = \"\"\n",
        "            page_text = page_text.strip()\n",
        "            if not page_text:\n",
        "                continue\n",
        "\n",
        "            # chunking\n",
        "            start = 0\n",
        "            while start < len(page_text):\n",
        "                end = start + chunk_size\n",
        "                chunk = page_text[start:end]\n",
        "                corpus.append({\n",
        "                    \"text\": chunk,\n",
        "                    \"source\": f\"{filename} - página {page_num+1}\"\n",
        "                })\n",
        "                start = end - chunk_overlap\n",
        "\n",
        "    return corpus\n",
        "\n",
        "\n",
        "corpus = load_pdfs_and_chunk(BASE_FOLDER)\n",
        "print(\"Chunks generados:\", len(corpus))\n",
        "print(corpus[0])\n",
        "# ============================================\n",
        "# VECTOREAR Y CREAR VECTORSTORE\n",
        "# ============================================\n",
        "\n",
        "EMB_MODEL = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "embeddings_lc = HuggingFaceEmbeddings(model_name=EMB_MODEL)\n",
        "\n",
        "docs_texts = [c[\"text\"] for c in corpus]\n",
        "docs_meta  = [{\"title\": c[\"source\"]} for c in corpus]\n",
        "\n",
        "vectorstore = FAISS.from_texts(\n",
        "    texts=docs_texts,\n",
        "    embedding=embeddings_lc,\n",
        "    metadatas=docs_meta\n",
        ")\n",
        "\n",
        "vectorstore.save_local(\"/content/faiss_index_creditos_agro\")\n",
        "print(\"Vectorstore creado correctamente.\")\n",
        "# ============================================\n",
        "# RERANK\n",
        "# ============================================\n",
        "USE_RERANK = True\n",
        "\n",
        "if USE_RERANK:\n",
        "    RERANK_MODEL = \"mixedbread-ai/mxbai-rerank-base-v1\"\n",
        "    rr_tok = AutoTokenizer.from_pretrained(RERANK_MODEL)\n",
        "    rr_model = AutoModelForSequenceClassification.from_pretrained(RERANK_MODEL)\n",
        "    rr_model = rr_model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\").eval()\n",
        "\n",
        "    def rerank(query, candidates, top_n=5):\n",
        "        pairs = [(query, c.page_content) for c in candidates]\n",
        "        enc = rr_tok(\n",
        "            [p[0] for p in pairs],\n",
        "            [p[1] for p in pairs],\n",
        "            truncation=True,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        enc = {k: v.to(rr_model.device) for k, v in enc.items()}\n",
        "        with torch.no_grad():\n",
        "            scores = rr_model(**enc).logits.squeeze(-1).float().cpu().numpy()\n",
        "        order = np.argsort(-scores)\n",
        "        return [candidates[i] for i in order[:top_n]]\n",
        "# ============================================\n",
        "# LLM GENERATIVO\n",
        "# ============================================\n",
        "\n",
        "LLM_MODEL = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "\n",
        "bnb_cfg = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
        "llm = AutoModelForCausalLM.from_pretrained(\n",
        "    LLM_MODEL,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_cfg\n",
        ")\n",
        "\n",
        "print(\"LLM cargado.\")\n",
        "# ============================================\n",
        "# FUNCIONES RAG\n",
        "# ============================================\n",
        "from textwrap import shorten\n",
        "\n",
        "def retrieve(query, k=10, search_type=\"similarity\"):\n",
        "    if search_type == \"similarity\":\n",
        "        return vectorstore.similarity_search(query, k=k)\n",
        "    elif search_type == \"mmr\":\n",
        "        # búsqueda con diversidad (Max Marginal Relevance)\n",
        "        return vectorstore.max_marginal_relevance_search(query, k=k, lambda_mult=0.1)\n",
        "    else:\n",
        "        # por si acaso\n",
        "        return vectorstore.similarity_search(query, k=k)\n",
        "\n",
        "def build_prompt(query, contexts):\n",
        "    header = (\n",
        "        \"Responde en español colombiano usando SOLO el contexto.\\n\"\n",
        "        \"Si no encuentras la respuesta en el contexto, dilo explícitamente.\\n\"\n",
        "        \"Sé claro y explica como si hablaras con un caficultor.\\n\\n\"\n",
        "    )\n",
        "    ctx = \"\"\n",
        "    for i, d in enumerate(contexts):\n",
        "        t = d.metadata.get(\"title\",\"\")\n",
        "        ctx += f\"[{i+1}] {d.page_content}\\n(Cita: {t})\\n\\n\"\n",
        "    user = f\"Pregunta: {query}\\n\\nCita las fuentes como [n] cuando corresponda.\"\n",
        "    return header + \"Contexto:\\n\" + ctx + user\n",
        "\n",
        "def generate_answer(prompt, max_new_tokens=400):\n",
        "    input_ids = tok(prompt, return_tensors=\"pt\").to(llm.device)\n",
        "    out = llm.generate(\n",
        "        **input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=0.2,\n",
        "        do_sample=False\n",
        "    )\n",
        "    return tok.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "def ask(query, k=10, rerank_top=5):\n",
        "    hits = retrieve(query, k=k)\n",
        "    if not hits:\n",
        "        return \"No encontré ninguna información relevante en los documentos.\"\n",
        "\n",
        "    if USE_RERANK:\n",
        "        hits = rerank(query, hits, top_n=rerank_top)\n",
        "\n",
        "    prompt = build_prompt(query, hits)\n",
        "    respuesta = generate_answer(prompt)\n",
        "\n",
        "    fuentes = \"\\n\".join([f\"[{i+1}] {shorten(h.metadata['title'],90)}\" for i,h in enumerate(hits)])\n",
        "    respuesta += \"\\n\\nFuentes:\\n\" + fuentes\n",
        "    return respuesta\n",
        "query = (\n",
        "    \"Soy pequeño caficultor en Pitalito y quiero comprar un dron de aspersión. \"\n",
        "    \"¿Qué líneas de crédito de inversión agropecuaria existen y qué plazos y tasas manejan?\"\n",
        ")\n",
        "\n",
        "print(ask(query, k=10, rerank_top=5))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}